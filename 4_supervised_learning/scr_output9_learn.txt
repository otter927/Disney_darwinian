[[[6.68815635e-01 6.33165812e-01 6.20075786e-01 ... 8.10972563e-01
   3.42265799e-01 6.87199141e-01]
  [9.96449338e-01 1.43237696e-01 2.68448267e-01 ... 4.20201109e-01
   1.78517213e-01 3.28931505e-01]
  [9.73286078e-01 9.72022780e-01 3.63215007e-01 ... 9.97007735e-01
   4.30541658e-02 2.70095526e-01]
  ...
  [2.84734178e-02 2.40156027e-01 2.21799923e-01 ... 1.76397501e-01
   3.69919399e-02 5.37125814e-01]
  [3.27065064e-01 4.36829607e-01 5.78459488e-01 ... 6.43334439e-01
   1.42334480e-01 1.64300690e-01]
  [7.90328840e-01 4.29343032e-01 5.67704222e-01 ... 3.75370533e-01
   4.38267788e-01 6.43929327e-01]]

 [[4.24833439e-01 4.79781175e-01 7.75696748e-01 ... 8.89435173e-01
   8.31206136e-01 1.97601163e-01]
  [5.12520273e-01 4.61806680e-01 8.39055650e-01 ... 2.62131734e-01
   1.00802190e-01 6.28720848e-01]
  [6.81525611e-01 1.65195469e-01 1.19340158e-01 ... 5.40577055e-01
   5.92935563e-01 8.53874219e-01]
  ...
  [7.95841211e-01 2.31590413e-01 6.22947939e-01 ... 9.08684702e-01
   2.55660047e-01 3.58462074e-01]
  [4.73592732e-01 2.75029928e-01 7.36757836e-02 ... 3.53254685e-01
   5.50117768e-01 5.25022707e-01]
  [4.27131015e-01 9.11437648e-01 8.17730587e-01 ... 7.96350395e-01
   2.13298621e-01 8.19823340e-01]]

 [[9.64835615e-01 7.76476793e-02 9.70441564e-01 ... 5.93558258e-01
   8.29989888e-01 4.54964210e-01]
  [3.50286322e-01 8.42746387e-01 7.82460413e-01 ... 7.21573939e-01
   7.21866149e-01 7.02007208e-01]
  [7.89860447e-01 8.13790471e-02 2.55160206e-01 ... 5.77942877e-01
   8.51912229e-01 9.27622056e-01]
  ...
  [7.83724929e-01 4.21023907e-01 5.21952758e-02 ... 3.19670976e-01
   2.10821642e-02 6.60937420e-01]
  [2.56282078e-01 8.59340229e-01 8.47861794e-01 ... 6.28150922e-01
   9.73185058e-01 2.75982689e-01]
  [8.47676578e-01 2.04631115e-01 7.80496379e-01 ... 8.35262276e-01
   3.17116228e-01 3.93505152e-01]]

 [[5.75686769e-04 4.16042444e-02 3.95543623e-01 ... 3.55760527e-01
   6.99619425e-01 9.26494004e-01]
  [4.40343434e-01 3.40963393e-01 3.70561987e-01 ... 3.85403747e-01
   2.21499702e-01 6.04475154e-01]
  [1.61453992e-01 8.75602769e-01 1.74956538e-01 ... 2.83403773e-01
   2.36423782e-01 3.53290421e-01]
  ...
  [1.77826628e-01 5.83049354e-01 6.75768713e-01 ... 4.62575842e-01
   6.18171466e-01 8.62261757e-01]
  [3.72464085e-01 7.48377294e-01 1.05788063e-02 ... 1.21317493e-01
   6.82513241e-01 3.49628660e-01]
  [8.33826811e-01 3.51386868e-01 2.26445647e-01 ... 2.91215422e-01
   3.72673205e-02 9.67321483e-02]]]
[[ 1 -1  1 -1  1  1 -1 -1 -1 -1 -1  1 -1 -1  1  1]
 [ 1 -1 -1 -1 -1 -1  1 -1  1  1  1 -1  1 -1 -1  1]
 [ 1  1  1 -1 -1  1 -1  1  1 -1  1 -1  1 -1 -1 -1]
 [ 1  1 -1 -1  1  1 -1 -1 -1  1  1  1  1 -1  1  1]]
train_ds
<BatchDataset element_spec=(TensorSpec(shape=(None, 40, 16), dtype=tf.float64, name=None), TensorSpec(shape=(None, 16), dtype=tf.int64, name=None))>
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn (SimpleRNN)      (None, 16)                528       
                                                                 
=================================================================
Total params: 528
Trainable params: 528
Non-trainable params: 0
_________________________________________________________________
x_mb
tf.Tensor(
[[ 1  1  1 -1 -1  1 -1  1  1 -1  1 -1  1 -1 -1 -1]
 [ 1 -1 -1 -1 -1 -1  1 -1  1  1  1 -1  1 -1 -1  1]], shape=(2, 16), dtype=int64)
tr_loss:[1.3924341 1.3989727]
x_mb
tf.Tensor(
[[ 1  1 -1 -1  1  1 -1 -1 -1  1  1  1  1 -1  1  1]
 [ 1 -1 -1 -1 -1 -1  1 -1  1  1  1 -1  1 -1 -1  1]], shape=(2, 16), dtype=int64)
tr_loss:[1.4115514 1.3516014]
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 437ms/step
[[ 0.34116647 -0.5665618   0.7976753  -0.37140292 -0.9300146   0.40843332
   0.950476   -0.13299112  0.5737141  -0.23976031  0.476313   -0.7533036
  -0.07490508 -0.85466295  0.7687259  -0.79946065]
 [ 0.46836418  0.02717094  0.32435158 -0.04580314 -0.5964058   0.06377368
   0.9225485   0.26739284 -0.07778725 -0.09618669  0.65094507 -0.47067717
  -0.79658175 -0.6388763   0.852627   -0.28792128]
 [-0.69972163 -0.27758053  0.42361724 -0.48652926 -0.4773095  -0.24481095
   0.78359914  0.51827383  0.60392135 -0.31209126  0.6742009  -0.35056552
   0.58629483 -0.71570307  0.86393094 -0.80803674]
 [ 0.64633113  0.14456561 -0.1996765  -0.5343278  -0.41872025  0.56460696
   0.6598089   0.21127807 -0.44991606 -0.61056226  0.26235253 -0.6769879
  -0.31549335 -0.7421214   0.6464448  -0.7138594 ]]
